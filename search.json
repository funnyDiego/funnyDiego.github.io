[{"title":"hadoop 伪分布式部署","url":"/2016/05/23/hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2/","content":"学习 hadoop 主要参考 hadoop官网，本文主要介绍伪分布式模式的搭建（单节点）及注意事项。\nhadoop版本：hadoop-3.2.2\nJava版本：jdk 1.8.0_45\n1.从官网下载 Hadoop 二进制文件https://hadoop.apache.org/releases.html 点击 3.2.2 版本 binary =&gt; 进入=&gt; https://dlcdn.apache.org/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz 下载 Hadoop 包\n2.上传到 ~/tmp 文件夹下（新建）3.创建用户 hadoop 和常规文件夹[root@bigdata ~]# useradd hadoop[root@bigdata ~]# id hadoopuid=1001(hadoop) gid=1002(hadoop) groups=1002(hadoop)[hadoop@bigdata ~]$ mkdir  app data lib script software source tmp[hadoop@bigdata ~]$ lltotal 28drwxrwxr-x 4 hadoop hadoop 4096 Dec  5 20:43 appdrwxrwxr-x 2 hadoop hadoop 4096 Nov 17 22:15 datadrwxrwxr-x 2 hadoop hadoop 4096 Nov 17 22:15 libdrwxrwxr-x 2 hadoop hadoop 4096 Nov 17 22:15 scriptdrwxrwxr-x 2 hadoop hadoop 4096 Nov 21 09:24 softwaredrwxrwxr-x 2 hadoop hadoop 4096 Nov 17 22:15 sourcedrwxrwxr-x 2 hadoop hadoop 4096 Dec  6 23:19 tmp\n\n\n\n4.将 ~/tmp/hadoop-3.2.2.tar.gz 拷贝到 hadoop 用户下的 software 文件夹root 用户下[root@bigdata tmp]# mv hadoop-3.2.2.tar.gz /home/hadoop/software/\n\n\n\n5.解压 tar 包到 ~/app 目录下hadoop用户（-C：解压到指定目录，解压到当前目录不需要该参数）[hadoop@bigdata software]$ tar -zxvf hadoop-3.2.2.tar.gz -C ../app/\n6.创建软连接[hadoop@bigdata app]$ ln -s hadoop-3.2.2 hadoop[hadoop@bigdata app]$ lltotal 8lrwxrwxrwx 1 hadoop hadoop   12 Nov 21 09:38 hadoop -&gt; hadoop-3.2.2drwxr-xr-x 9 hadoop hadoop 4096 Dec  7 21:20 hadoop-3.2.2\n\n\n\n7.配置 JAVA_HOME[hadoop@bigdata app]$ cd hadoop/etc/hadoop[hadoop@bigdata app]$ ll[hadoop@bigdata hadoop]$ pwd/home/hadoop/app/hadoop/etc/hadoop编辑配置文件，新增 JAVA_HOME [hadoop@bigdata app]$ vi hadoop-env.shJAVA_HOME=/usr/java/jdk1.8.0_45\n\n\n\n8.单机模式测试（无进程，用以debug）[hadoop@bigdata hadoop]$ cd ../../[hadoop@bigdata hadoop]$ lsbin  etc  hadoop-3.2.2  include  lib  libexec  LICENSE.txt  NOTICE.txt  README.txt  sbin  share[hadoop@bigdata hadoop]$ mkdir input[hadoop@bigdata hadoop]$ cp etc/hadoop/*.xml input[hadoop@bigdata hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#x27;dfs[a-z.]+&#x27;[hadoop@bigdata hadoop]$ cd output/[hadoop@bigdata output]$ lspart-r-00000  _SUCCESS[hadoop@bigdata output]$ cat part-r-00000 1\tdfsadmin\n\n\n\n9.伪分布部署1）配置文件\n[hadoop@bigdata hadoop]$ vi etc/hadoop/core-site.xml\n\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at```http://www.apache.org/licenses/LICENSE-2.0```  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://bigdata:9000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;\n\n\n[hadoop@bigdata hadoop]$ vi hdfs-site.xml\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at```http://www.apache.org/licenses/LICENSE-2.0```  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;property&gt;          &lt;name&gt;dfs.replication&lt;/name&gt;          &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;\n\n\n\n**2）配置 ssh 无密码登录（重要） **\n[hadoop@bigdata ~]$ ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): /home/hadoop/.ssh/id_rsa already exists.Overwrite (y/n)? [hadoop@bigdata ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@bigdata ~]$ chmod 0600 ~/.ssh/authorized_keys  ### 重中之重（赋予权限）[hadoop@bigdata ~]$ ll .ssh/authorized_keys -rw------- 1 hadoop hadoop 568 Nov 21 10:28 .ssh/authorized_keysssh 登录是相同的，如果没有写用户，默认是光标所在的用户，当前为 hadoop[hadoop@bigdata ~]$ ssh bigdata[hadoop@bigdata ~]$ ssh hadoop@bigdata[hadoop@bigdata ~]$ ssh bigdataWelcome to Alibaba Cloud Elastic Compute Service !Last login: Tue Dec  7 21:20:21 2021[hadoop@bigdata ~]$ 如上，则无密码登录设置成功。\n\n\n\n3）格式化[hadoop@bigdata hadoop]$ bin/hdfs namenode -format\n...2021-12-07 23:41:21,660 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1768541083-172.30.73.2-16388916816522021-12-07 23:41:21,675 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted.2021-12-07 23:41:21,699 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression2021-12-07 23:41:21,791 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .2021-12-07 23:41:21,805 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 02021-12-07 23:41:21,810 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.2021-12-07 23:41:21,810 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at bigdata/172.30.73.2************************************************************/\n\n\n\n4）启动 NameNode 和 DataNode\n[hadoop@bigdata hadoop]$ sbin/start-dfs.sh Starting namenodes on [bigdata]Starting datanodeslocalhost: Warning: Permanently added &#x27;localhost&#x27; (ECDSA) to the list of known hosts.Starting secondary namenodes [bigdata]\n\n查看进程是否启动成功，jps，但是不保险，最好使用 ps -ef 查看为准\n[hadoop@bigdata hadoop]$ jps1028139 DataNode1028010 NameNode1028375 SecondaryNameNode1028533 Jps[hadoop@bigdata hadoop]$ ps -ef | grep hadoophadoop   1028010       1  2 23:47 ?        00:00:07 /usr/java/jdk1.8.0_45/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dyarn.log.dir=/home/hadoop/app/hadoo-3.2.2/logs -Dyarn.log.file=hadoop-hadoop-namenode-bigdata.log -Dyarn.home.dir=/home/hadoop/app/hadoop-3.2.2 -Dyarn.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-3.2.2/lib/native -Dhadoop.log.dir=/home/hadoop/app/hadoo-3.2.2/logs -Dhadoop.log.file=hadoop-hadoop-namenode-bigdata.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-3.2.2 -Dhadoo.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.namenode.NameNodehadoop   1028139       1  1 23:47 ?        00:00:06 /usr/java/jdk1.8.0_45/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/home/hadoop/app/hadoop-3.2.2/logs -Dyarn.log.file=hadoop-hadoop-datanode-bigdata.log -Dyarn.home.dir=/home/hadoop/app/hadoop-3.2.2 -Dyarn.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-3.2.2/lib/native -Dhadoop.log.dir=/home/hadoop/app/hadoop-3.2.2/logs -Dhadoop.log.file=hadoophadoop-datanode-bigdata.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-3.2.2 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNodehadoop   1028375       1  1 23:47 ?        00:00:04 /usr/java/jdk1.8.0_45/bin/java -Dproc_secondarynamenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dyarn.log.dir=/home/hadoop/app/hadoop-3.2.2/logs -Dyarn.log.file=hadoop-hadoop-secondarynamenode-bigdata.log -Dyarn.home.dir=/home/hadoop/app/hadoop-3.2.2 -Dyarn.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop-3.2.2/lib/native -Dhadoop.log.dir=/home/hadoop/app/hadoop-3.2.2/logs -Dhadoop.log.file=hadoop-hadoop-secondarynamenode-bigdata.log -Dhadoop.home.dir=/home/hadoop/app/hadoop-3.2.2 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n\n\n\n5）访问 Web 界面    hadoop 启动后，访问 hdfs 系统：http://localhost:9870/    根据实际使用的云服务器地址： http://公网IP:9870\n6）创建 HDFS 文件夹，执行 Mapreduce Job（形成 hdfs 文件系统，不同于 linux 文件系统）\n[hadoop@bigdata hadoop]$ bin/hdfs dfs -mkdir /user[hadoop@bigdata hadoop]$ bin/hdfs dfs -ls /Found 1 itemsdrwxr-xr-x   - hadoop supergroup          0 2021-12-08 00:07 /user[hadoop@bigdata hadoop]$ bin/hdfs dfs -mkdir /user/hadoop[hadoop@bigdata hadoop]$ bin/hdfs dfs -mkdir input前面没有绝对路径，默认的是当前用户的家目录下面[hadoop@bigdata hadoop]$ bin/hdfs dfs -ls /user/hadoop/Found 1 itemsdrwxr-xr-x   - hadoop supergroup          0 2021-12-08 00:09 /user/hadoop/input\n\n\n\n7）拷贝文件到 hdfs 上，执行 mapreduce job\n[hadoop@bigdata hadoop]$ bin/hdfs dfs -put etc/hadoop/*.xml input[hadoop@bigdata hadoop]$ bin/hdfs dfs -ls /user/hadoop/inputFound 9 items-rw-r--r--   1 hadoop supergroup       9213 2021-12-08 00:12 /user/hadoop/input/capacity-scheduler.xml-rw-r--r--   1 hadoop supergroup        882 2021-12-08 00:12 /user/hadoop/input/core-site.xml-rw-r--r--   1 hadoop supergroup      11392 2021-12-08 00:12 /user/hadoop/input/hadoop-policy.xml-rw-r--r--   1 hadoop supergroup        871 2021-12-08 00:12 /user/hadoop/input/hdfs-site.xml-rw-r--r--   1 hadoop supergroup        620 2021-12-08 00:12 /user/hadoop/input/httpfs-site.xml-rw-r--r--   1 hadoop supergroup       3518 2021-12-08 00:12 /user/hadoop/input/kms-acls.xml-rw-r--r--   1 hadoop supergroup        682 2021-12-08 00:12 /user/hadoop/input/kms-site.xml-rw-r--r--   1 hadoop supergroup        758 2021-12-08 00:12 /user/hadoop/input/mapred-site.xml-rw-r--r--   1 hadoop supergroup        690 2021-12-08 00:12 /user/hadoop/input/yarn-site.xml\n\n提交到服务器上执行 mapreduce job[hadoop@bigdata hadoop]$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output &#39;dfs[a-z.]+&#39;\n将执行后的结果文件夹下载到 linux 系统（bin/hdfs dfs -get 命令）[hadoop@bigdata hadoop]$ lsbin  etc  hadoop-3.2.2  include  input  lib  libexec  LICENSE.txt  logs  NOTICE.txt  README.txt  sbin  share[hadoop@bigdata hadoop]$  bin/hdfs dfs -get output output[hadoop@bigdata hadoop]$ lsbin  etc  hadoop-3.2.2  include  input  lib  libexec  LICENSE.txt  logs  NOTICE.txt  output  README.txt  sbin  share[hadoop@bigdata hadoop]$ ll output/total 4-rw-r--r-- 1 hadoop hadoop 29 Dec  8 00:19 part-r-00000-rw-r--r-- 1 hadoop hadoop  0 Dec  8 00:19 _SUCCESS[hadoop@bigdata hadoop]$ cat output/*1\tdfsadmin1\tdfs.replication相较于单机模式的运行结果，这里多了一个 dfs.replication，是因为 xml 配置文件里新增了副本设置。\n\n","tags":["hadoop"]},{"title":"hadoop 探索","url":"/2016/05/25/hadoop%E6%8E%A2%E7%B4%A2/","content":"学习 Hadoop 主要参考 hadoop官网，在实际生产过程中，注意修改 hadoop 默认的配置：启动进程时以 hostname 启用、修改配置里的默认地址。\nHadoop 版本：hadoop-3.2.2\nJava 版本：jdk 1.8.0_45\n1、以 hostname 启动 hadoop 进程好处：当服务器发生变更时，只需要修改 hosts 文件即可，不需要一个一个地修改服务的配置文件。\n比如：以 bigdata（服务器的 hostname）启动 NN（namenode），DN（datanode），SNN（secondary namenode）\n对应修改：\n1）namenode 以 bigdata 启动：修改文件 core-site.xml\n[hadoop@bigdata hadoop]$ vi app/hadoop/etc/hadoop/core-site.xml\t&lt;configuration&gt;\t    &lt;property&gt;\t        &lt;name&gt;fs.defaultFS&lt;/name&gt;\t        &lt;value&gt;hdfs://bigdata:9000&lt;/value&gt;\t    &lt;/property&gt;\t&lt;/configuration&gt;\n\n2）datanode 以 bigdata 启动：修改文件 workers\n[hadoop@bigdata hadoop]$ vi app/hadoop/etc/hadoop/workers\tbigdata\n\n3）secondary namenode 以 bigdata 启动：修改文件 hdfs-site.xml\n[hadoop@bigdata hadoop]$ vi app/hadoop/etc/hadoop/hdfs-site.xml\t&lt;configuration&gt;\t    &lt;property&gt;\t        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\t        &lt;value&gt;bigdata:9868&lt;/value&gt;\t    &lt;/property&gt;\t&lt;/configuration&gt;    &lt;configuration&gt;        &lt;property&gt;            &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;            &lt;value&gt;bigdata:9869&lt;/value&gt;        &lt;/property&gt;    &lt;/configuration&gt;\n\n\n\n2、hadoop 默认 tmp 目录修改从官网 core-default.xml 配置文件中查找 hadoop.tmp.dir 对应的值：**/tmp/hadoop-${user.name}**\n固定语法：hadoop-${user.name}，表示不同用户，e.g. hadoop-hadoop\n查看 /tmp 目录：\n[hadoop@bigdata tmp]$ lltotal 80drwxrwxr-x 3 hadoop hadoop 4096 Jan 22 23:22 hadoop-hadoop-rw-rw-r-- 1 hadoop hadoop    8 Jan 24 18:01 hadoop-hadoop-datanode.pid-rw-rw-r-- 1 hadoop hadoop    8 Jan 24 18:01 hadoop-hadoop-namenode.pid-rw-rw-r-- 1 hadoop hadoop    8 Jan 24 18:01 hadoop-hadoop-secondarynamenode.piddrwxr-xr-x 3 hadoop hadoop 4096 Jan 22 22:19 hadoop-yarn-hadoopdrwxr-xr-x 2 hadoop hadoop 4096 Jan 24 18:01 hsperfdata_hadoopdrwxr-xr-x 2 root   root   4096 Jan 20 17:01 hsperfdata_rootdrwxrwxr-x 2 hadoop hadoop 4096 Jan 22 22:30 jetty-0_0_0_0-9868-secondary-_-any-1077432879358563547.dirdrwxrwxr-x 2 hadoop hadoop 4096 Jan 22 23:22 jetty-0_0_0_0-9868-secondary-_-any-1977629256625793193.dir\n\n其中 hadoop-hadoop 则是默认的 hadoop 文件存储目录，包括 data，namenode 等；\n*.pid 文件：记录的是进程的端口号，控制进程\n\n 进程启动会写一个 pid 文件；\n 进程停止会从 pid 文件读取进程号，然后 kill 掉。\n\n[hadoop@bigdata tmp]$ cat hadoop-hadoop-datanode.pid 1289538[hadoop@bigdata tmp]$ cat hadoop-hadoop-namenode.pid 1289409[hadoop@bigdata tmp]$ ps -ef | grep 1289409hadoop   1289409       1  0 Jan24 ?        00:00:50 /usr/java/jdk1.8.0_45/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dyarn.log.dir=/home/hadoop/app/hadoop/logs -Dyarn.log.file=hadoop-hadoop-namenode-bigdata.log -Dyarn.home.dir=/home/hadoop/app/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/home/hadoop/app/hadoop/lib/native -Dhadoop.log.dir=/home/hadoop/app/hadoop/logs -Dhadoop.log.file=hadoop-hadoop-namenode-bigdata.log -Dhadoop.home.dir=/home/hadoop/app/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.namenode.NameNodehadoop   1293184 1293071  0 09:42 pts/0    00:00:00 grep --color=auto 1289409\n\n\n\n如果生产仍使用默认 /tmp 目录，当 30 天后，linux 系统会自动清除数据，这里的 *.pid 文件可能会被删除，会导致：\n生产更新配置或 jar 包，DN/NN 等看似重启后生效了，但实际并没有重启成功，pid 未变。\n\n\n\n数据目录也在 /tmp/hadoop-hadoop 目录下，也很危险：\n[hadoop@bigdata dfs]$ pwd/tmp/hadoop-hadoop/dfs[hadoop@bigdata dfs]$ lltotal 12drwx------ 3 hadoop hadoop 4096 Jan 24 18:01 datadrwxrwxr-x 3 hadoop hadoop 4096 Jan 25 10:10 namedrwxrwxr-x 3 hadoop hadoop 4096 Jan 25 10:10 namesecondary\n\n\n\n基于以上情况，生产环境需要注意修改配置：\n2.1 hadoop /tmp 目录的修改hadoop 存储目录的设置：core-site.xml\n[hadoop@bigdata hadoop]$ vi core-site.xml\t&lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/home/hadoop/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt;    &lt;/property&gt;\n\n[ 注意：/home/hadoop/tmp/ 替换的目录是 /tmp ]\n2.2 pid 文件存储目录修改hadoop-env.sh，新增：\n[hadoop@bigdata hadoop]$ vi hadoop-env.sh# where pid files are stored, /tmp by default.export HADOOP_PID_DIR=/home/hadoop/tmp\n\n2.3 datanode(DN) 的存储目录修改 ***hdfs-site.xml 【！！！】默认的存储地址：file://${hadoop.tmp.dir}/dfs/data ==&gt; /home/hadoop/tmp/hadoop-hadoop/dfs/data (伪分布式)\n生产上：因数据量巨大，会存在多块磁盘，此时设置：    (dfs.datanode.data.dir: 10 块物理磁盘)\ndfs.datanode.data.dir: /data01/dfs/dn, /data02/dfs/dn, …,/data10/dfs/dn\n​    也就是说：假设一块磁盘的写能力 30 M/s, 10 块磁盘则是 300 M/s，一个 300 M 的数据则只需要 1 秒即可完成写入\n​    =&gt; 生产上，多块磁盘是为了更大的存储空间，且更高效的读写 IO\n[hadoop@bigdata hadoop]$ vi hdfs-site.xml\t&lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;/data01/dfs/dn, /data02/dfs/dn, ...,/data10/dfs/dn&lt;/value&gt;    &lt;/property&gt;\n\n2.4 namenode(NN) 存储目录修改hdfs-site.xml：\n默认的存储地址：file://${hadoop.tmp.dir}/dfs/name ==&gt; /home/hadoop/tmp/hadoop-hadoop/dfs/name (伪分布式)\n[hadoop@bigdata hadoop]$ vi hdfs-site.xml\t&lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;/home/hadoop/tmp/dfs/name&lt;/value&gt;    &lt;/property&gt;\n\n","tags":["hadoop"]},{"title":"mysql 二进制部署","url":"/2015/09/18/mysql%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/","content":"本文介绍 MySQL 的二进制部署，参考资料：https://github.com/Hackeruncle/mysql。\nMySQL 版本：5.6.23\n1.上传下载好的tar包rz mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz\n2.移动到 /usr/local 目录下mv ./mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz /usr/local/\n3.查看是否已经安装MySQL（比如：系统自带）[root@bigdata local]# ps -ef | grep mysqlroot     1021625  959563  0 22:39 pts/1    00:00:00 grep --color=auto mysqlrpm -qa | grep -i mysql\n\n\n\n4.tar包解压，创建软连接[root@bigdata local]# tar xzvf mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz[root@bigdata local]# ln -s mysql-5.6.23-linux-glibc2.5-x86_64 mysql[root@bigdata local]# ll\n\n\n\n5.创建 user 和 group[root@bigdata local]# groupadd -g 101 dba[root@bigdata local]# useradd -u 514 -g dba -G root -d /usr/local/mysql mysqladminuseradd: warning: the home directory already exists.Not copying any file from skel directory into it.\n\n\n\n6.创建 MySQL 配置文件vi /etc/my.cnf\n进入编辑模式，粘贴下述类容（生产环境调优时，修改 innodb_buffer_pool_size = 2048M 即可，一般6G、8G、12G 就足以）\n[client]port            = 3306socket          = /usr/local/mysql/data/mysql.sock[mysqld]port            = 3306socket          = /usr/local/mysql/data/mysql.sockskip-external-lockingkey_buffer_size = 256Msort_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 4Mquery_cache_size= 32Mmax_allowed_packet = 16Mmyisam_sort_buffer_size=128Mtmp_table_size=32Mtable_open_cache = 512thread_cache_size = 8wait_timeout = 86400interactive_timeout = 86400max_connections = 600# Try number of CPU&#x27;s*2 for thread_concurrencythread_concurrency = 32#isolation level and default engine default-storage-engine = INNODBtransaction-isolation = READ-COMMITTEDserver-id  = 1basedir     = /usr/local/mysqldatadir     = /usr/local/mysql/datapid-file     = /usr/local/mysql/data/hostname.pid#open performance schemalog-warningssysdate-is-nowbinlog_format = MIXEDlog_bin_trust_function_creators=1log-error  = /usr/local/mysql/data/hostname.errlog-bin=/usr/local/mysql/arch/mysql-bin#other logs#general_log =1#general_log_file  = /usr/local/mysql/data/general_log.err#slow_query_log=1#slow_query_log_file=/usr/local/mysql/data/slow_log.err#for replication slave#log-slave-updates #sync_binlog = 1#for innodb options innodb_data_home_dir = /usr/local/mysql/data/innodb_data_file_path = ibdata1:500M:autoextendinnodb_log_group_home_dir = /usr/local/mysql/archinnodb_log_files_in_group = 2innodb_log_file_size = 200M# tun prodinnodb_buffer_pool_size = 2048Minnodb_additional_mem_pool_size = 50Minnodb_log_buffer_size = 16Minnodb_lock_wait_timeout = 100#innodb_thread_concurrency = 0innodb_flush_log_at_trx_commit = 1innodb_locks_unsafe_for_binlog=1#innodb io features: add for mysql5.5.8performance_schemainnodb_read_io_threads=4innodb-write-io-threads=4innodb-io-capacity=200#purge threads change default(0) to 1 for purgeinnodb_purge_threads=1innodb_use_native_aio=on#case-sensitive file names and separate tablespaceinnodb_file_per_table = 1lower_case_table_names=1[mysqldump]quickmax_allowed_packet = 16M[mysql]no-auto-rehash[mysqlhotcopy]interactive-timeout[myisamchk]key_buffer_size = 256Msort_buffer_size = 256Mread_buffer = 2Mwrite_buffer = 2M\n\n\n保存，退出，该配置文件已创建成功\n7.改变配置文件权限，第一次安装[root@bigdata local]# chown mysqladmin:dba /etc/my.cnf[root@bigdata local]# chmod 640 /etc/my.cnf[root@bigdata local]# chown -R mysqladmin:dba /usr/local/mysql[root@bigdata local]# chmod -R 755 /usr/local/mysql[root@bigdata local]# chown -R mysqladmin:dba /usr/local/mysql-5.6.23-linux-glibc2.5-x86_64[root@bigdata local]# su - mysqladmin[mysqladmin@bigdata ~]$ pwd/usr/local/mysql\n\n新建放置 binlog 文件夹 arch[mysqladmin@bigdata ~]$ mkdir arch\n执行命令 scripts/mysql_install_db\n[mysqladmin@bigdata ~]$ scripts/mysql_install_dbInstalling MySQL system tables..../bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory\n\n退到 root 用户安装 libaio[root@bigdata local]# yum -y install libaio\n再次安装 \n[mysqladmin@bigdata ~]$ scripts/mysql_install_db --user=mysqladmin --basedir=/usr/local/mysql --datadir=/usr/local/mysql/dataInstalling MySQL system tables...2021-12-05 23:20:09 0 [Warning] &#x27;THREAD_CONCURRENCY&#x27; is deprecated and will be removed in a future release.2021-12-05 23:20:09 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).OKFilling help tables...2021-12-05 23:20:20 0 [Warning] &#x27;THREAD_CONCURRENCY&#x27; is deprecated and will be removed in a future release.2021-12-05 23:20:20 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).OKTo start mysqld at boot time you have to copysupport-files/mysql.server to the right place for your systemPLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER !To do so, start the server, then issue the following commands:  /usr/local/mysql/bin/mysqladmin -u root password &#x27;new-password&#x27;  /usr/local/mysql/bin/mysqladmin -u root -h bigdata password &#x27;new-password&#x27;Alternatively you can run:  /usr/local/mysql/bin/mysql_secure_installationwhich will also give you the option of removing the testdatabases and anonymous user created by default.  This isstrongly recommended for production servers.See the manual for more instructions.You can start the MySQL daemon with:  cd . ; /usr/local/mysql/bin/mysqld_safe &amp;You can test the MySQL daemon with mysql-test-run.pl  cd mysql-test ; perl mysql-test-run.plPlease report any problems at http://bugs.mysql.com/The latest information about MySQL is available on the web at  http://www.mysql.comSupport MySQL by buying support/licenses at http://shop.mysql.comWARNING: Found existing config file /usr/local/mysql/my.cnf on the system.Because this file might be in use, it was not replaced,but was used in bootstrap (unless you used --defaults-file)and when you later start the server.The new default config file was created as /usr/local/mysql/my-new.cnf,please compare it with your file and take the changes you need.WARNING: Default config file /etc/my.cnf exists on the systemThis file will be read by default by the MySQL serverIf you do not want to use this, either remove it, or use the--defaults-file argument to mysqld_safe when starting the server\n\n\n\n8.配置 mysql 服务，设置开机自启动root 用户下：\n将服务文件拷贝到init.d下，并重命名为mysql\n[root@bigdata mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysql\n赋予可执行权限\n[root@bigdata mysql]# chmod +x /etc/rc.d/init.d/mysql[root@bigdata mysql]# ll /etc/rc.d/init.d/mysql-rwxr-xr-x 1 root root 10880 Dec  5 23:27 /etc/rc.d/init.d/mysql\n如果之前部署过 mysql 服务，可以删除服务\n[root@bigdata mysql]# chkconfig --del mysql\n第一次安装时，添加服务\n[root@bigdata mysql]# chkconfig --add mysql[root@bigdata mysql]# chkconfig --level 345 mysql on\n[root@bigdata mysql]# vi /etc/rc.local\n进入编辑模式，新增内容：\nsu - mysqladmin -c &quot;/etc/init.d/mysql start --federated&quot;\n9.启动 mysql 服务，查看服务和监听状态切换用户 mysqladmin\n[root@bigdata mysql]# su - mysqladmin[mysqladmin@bigdata ~]$ rm -rf my.cnf[mysqladmin@bigdata ~]$ which mysqld_safe~/bin/mysqld_safe\n\n启动 mysql 服务和查看 mysql 服务状态\n[mysqladmin@bigdata ~]$ service mysql startStarting MySQL.                                            [  OK  ][mysqladmin@bigdata ~]$ service mysql statusMySQL running (1022830)                                    [  OK  ]\n\n登录 mysql\n[mysqladmin@bigdata ~]$ mysql -uroot -pEnter password: Welcome to the MySQL monitor.  Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt;\n\n\n\n查看 mysql 进程，获取 PID：1022889\n[root@bigdata local]#  ps -ef | grep mysqldmysqlad+ 1022153       1  0 23:40 pts/0    00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --datadir=/usr/local/mysql/data --pid-file=/usr/local/mysql/data/hostname.pidmysqlad+ 1022830 1022153  0 23:40 pts/0    00:00:00 /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --plugin-dir=/usr/local/mysql/lib/plugin --log-error=/usr/local/mysql/data/hostname.err --pid-file=/usr/local/mysql/data/hostname.pid --socket=/usr/local/mysql/data/mysql.sock --port=3306root     1022889  959563  0 23:43 pts/1    00:00:00 grep --color=auto mysqld\n\n\n\n查看端口号：3306\n``[root@bigdata local]# netstat -ntlp | grep 1022830 tcp6       0      0 :::3306                 :::*                    LISTEN      1022830/mysqld \n10.登录 mysql[mysqladmin@bigdata ~]$ mysql\n有密码的时候\n[mysqladmin@bigdata ~]$ mysql -uroot -ppassword:Welcome to the MySQL monitor.  Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.6.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; show databases;  -- 此处是 4 个数据库+--------------------+| Database           |+--------------------+| information_schema || mysql              || performance_schema || test               |+--------------------+4 rows in set (0.00 sec)mysql&gt; use test;Database changedmysql&gt; show tables;Empty set (0.00 sec)\n\n\n\n11.修改密码和清理用户mysql&gt; use mysql;Database changedmysql&gt; show tables;+---------------------------+| Tables_in_mysql           |+---------------------------+| columns_priv              || db                        || event                     || func                      || general_log               || help_category             || help_keyword              || help_relation             || help_topic                || innodb_index_stats        || innodb_table_stats        || ndb_binlog_index          || plugin                    || proc                      || procs_priv                || proxies_priv              || servers                   || slave_master_info         || slave_relay_log_info      || slave_worker_info         || slow_log                  || tables_priv               || time_zone                 || time_zone_leap_second     || time_zone_name            || time_zone_transition      || time_zone_transition_type || user                      |+---------------------------+28 rows in set (0.00 sec)mysql&gt; select user,password,host from user;+------+----------+-----------+| user | password | host      |+------+----------+-----------+| root |          | localhost || root |          | bigdata   || root |          | 127.0.0.1 || root |          | ::1       ||      |          | localhost ||      |          | bigdata   |+------+----------+-----------+6 rows in set (0.00 sec)mysql&gt; update user set password=password(&#x27;diego@2012&#x27;) where user=&#x27;root&#x27;;Query OK, 4 rows affected (0.00 sec)Rows matched: 4  Changed: 4  Warnings: 0mysql&gt; select host,user,password from user;+-----------+------+-------------------------------------------+| host      | user | password                                  |+-----------+------+-------------------------------------------+| localhost | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || bigdata   | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || 127.0.0.1 | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || ::1       | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || localhost |      |                                           || bigdata   |      |                                           |+-----------+------+-------------------------------------------+6 rows in set (0.00 sec)mysql&gt; delete from user where user=&#x27;&#x27;;Query OK, 2 rows affected (0.00 sec)mysql&gt; select host,user,password from user;+-----------+------+-------------------------------------------+| host      | user | password                                  |+-----------+------+-------------------------------------------+| localhost | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || bigdata   | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || 127.0.0.1 | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 || ::1       | root | *90F81E137FE350CE49222B0DB5A9DB85EC51CF07 |+-----------+------+-------------------------------------------+4 rows in set (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)\n\n\n\n12.修改配置文件 .bashrc切换到 mysqladmin 用户，编辑配置文件 ~/.bashrc[mysqladmin@bigdata ~]$ vi .bashrc新增环境变量配置export MYSQL_HOME=/usr/local/mysqlexport PATH=$MYSQL_HOME/bin:$PATH\n[mysqladmin@bigdata ~]$ source .bashrc[mysqladmin@bigdata ~]$ echo $MYSQL_HOME/usr/local/mysql\n\n附加修改配置文件 ~/.bashrc\n[mysqladmin@bigdata ~]$ vi .bashrc\n新增如下配置PS1=uname -n&quot;:&quot;&#39;$USER&#39;&quot;:&quot;&#39;$PWD&#39;&quot;:&gt;&quot;; export PS1\n效果如下：\n[root@bigdata ~]# su - mysqladminLast login: Mon Dec  6 21:44:05 CST 2021 on pts/0bigdata:mysqladmin:/usr/local/mysql:&gt;llbigdata:mysqladmin:/usr/local/mysql:&gt;bigdata:mysqladmin:/usr/local/mysql:&gt;bigdata:mysqladmin:/usr/local/mysql:&gt;bigdata:mysqladmin:/usr/local/mysql:&gt;\n\n可直观地查看当前所在的目录。\n\n重新部署如果部署出了问题1）检查执行的目录、用户2）重新部署    i) rm -rf arch/*        binlog 文件，为了保持有效期，会有自动删除机制    ii) rm -rf data/*        数据    iii) 再次从执行下述命令开始：scripts/mysql_install_db –user=mysqladmin –basedir=/usr/local/mysql –datadir=/usr/local/mysql/data\n","tags":["mysql"]},{"title":"resource manager 默认端口8088被黑客入侵","url":"/2016/05/27/resource%20manager%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A38088%E8%A2%AB%E9%BB%91%E5%AE%A2%E5%85%A5%E4%BE%B5/","content":"yarn 启动后 RM（resource manager）的默认端口 8088 被黑客入侵，植入木马程序，侵占服务器资源：CPU 使用率达 600%，登陆服务器、命令执行非常缓慢。\n万恶的病毒、木马程序啊，害人不浅~~~\n解决办法\n找到挖矿程序对应的执行文件，e.g. dfhslf.sh，给这个文件赋权\n\nchmod 000 dfhslf.sh\n\n\n\n\n找到执行文件，修改文件内容 清空内容 或在文件最前添加： exit 0\n\n当然，在配置 yarn 的时候，也可以修改默认端口以预防：\n修改配置文件：yarn-site.xml\n[hadoop@bigdata hadoop]$ vi yarn-site.xml\t&lt;property&gt;\t    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\t    &lt;value&gt;bigdata:8269&lt;/value&gt;\t&lt;/property&gt;这里的端口即前端访问页面端口：http://bigdata:8269\n\n \n","tags":["hadoop","yarn"]},{"title":"yarn 的启动与配置","url":"/2016/05/25/yarn%E5%90%AF%E5%8A%A8%E4%B8%8E%E9%85%8D%E7%BD%AE/","content":"学习 Hadoop 主要参考 hadoop官网，在 hadoop NN/DN/SNN 启动后，启动 yarn 进程：\nResourceManager\t\tRMNodeManager\t\t\tNM\n\n提交 yarn 任务，进行 wc 统计。\nHadoop 版本：hadoop-3.2.2\nJava 版本：jdk 1.8.0_45\n1、配置 mapred-site.xml, yarn-site.xml[hadoop@bigdata hadoop]$ vi mapred-site.xml\t&lt;configuration&gt;\t    &lt;property&gt;\t        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\t        &lt;value&gt;yarn&lt;/value&gt;\t    &lt;/property&gt;\t    &lt;property&gt;\t        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;\t        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;\t    &lt;/property&gt;\t&lt;/configuration&gt;[hadoop@bigdata hadoop]$ vi yarn-site.xml\t&lt;configuration&gt;\t    &lt;property&gt;\t        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\t        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\t    &lt;/property&gt;\t    &lt;property&gt;\t        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;\t     &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;\t    &lt;/property&gt;\t&lt;/configuration&gt;\n\n\n\n2 启动 RM 和 NM 进程：[hadoop@bigdata hadoop]$ pwd/home/hadoop/app/hadoop[hadoop@bigdata hadoop]$ sbin/start-yarn.sh Starting resourcemanagerStarting nodemanagers[hadoop@bigdata hadoop]$ \n\n\n\n3 浏览 RM 的 web 页面：ResourceManager - http://localhost:8088/\n[注意] 开放阿里云服务器的安全组\n4 案例测试： word count, wc, 词频统计4.1 ！！配置 HADOOP_HOME[hadoop@bigdata ~]$ vi .bashrcexport HADOOP_HOME=/home/hadoop/app/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH[hadoop@bigdata ~]$ source .bashrc[hadoop@bigdata ~]$ which hdfs~/app/hadoop/bin/hdfs\n\nhdfs 存储路劲与 linux 路径是单独的：\n[hadoop@bigdata ~]$ hdfs dfs -ls /Found 2 itemsdrwxr-xr-x   - hadoop supergroup          0 2022-01-24 12:59 /hdfsapidrwxr-xr-x   - hadoop supergroup          0 2022-01-24 13:40 /user[hadoop@bigdata ~]$ ls /bin  boot  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n\n4.2 准备数据文件[hadoop@bigdata ~]$ vi 1.log[hadoop@bigdata ~]$ cat 1.log Diego Jodiehello world countspark a b c moon Diego Jodie Jodiespark[hadoop@bigdata ~]$ hdfs dfs -put 1.log /input[hadoop@bigdata ~]$ hdfs dfs -cat /input/1.logDiego Jodiehello world countspark a b c moon Diego Jodie Jodiespark[hadoop@bigdata ~]$\n\n4.3 查看测试用的 jar 包：example | find -name[hadoop@bigdata hadoop]$ cd ~/app/hadoop[hadoop@bigdata hadoop]$ find ./ -name &#x27;*example*&#x27;./share/doc/hadoop/hadoop-mapreduce-examples./share/doc/hadoop/hadoop-auth-examples./share/doc/hadoop/hadoop-yarn/hadoop-yarn-common/apidocs/org/apache/hadoop/yarn/webapp/example./share/hadoop/yarn/yarn-service-examples./share/hadoop/mapreduce/lib-examples./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar./share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-3.2.2-test-sources.jar./share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-3.2.2-sources.jar./etc/hadoop/shellprofile.d/example.sh./etc/hadoop/ssl-client.xml.example./etc/hadoop/ssl-server.xml.example./etc/hadoop/hadoop-user-functions.sh.example./lib/native/examples./libexec/hadoop-layout.sh.example./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar\n\n执行 yarn 任务：\n[hadoop@bigdata hadoop]$ yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jarAn example program must be given as the first argument.Valid program names are:  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.  dbcount: An example job that count the pageview counts from a database.  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.  grep: A map/reduce program that counts the matches of a regex in the input.  join: A job that effects a join over sorted, equally partitioned datasets  multifilewc: A job that counts words from several files.  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.  randomwriter: A map/reduce program that writes 10GB of random data per node.  secondarysort: An example defining a secondary sort to the reduce.  sort: A map/reduce program that sorts the data written by the random writer.  sudoku: A sudoku solver.  teragen: Generate data for the terasort  terasort: Run the terasort  teravalidate: Checking results of terasort  wordcount: A map/reduce program that counts the words in the input files.  wordmean: A map/reduce program that counts the average length of the words in the input files.  wordmedian: A map/reduce program that counts the median length of the words in the input files.  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.[hadoop@bigdata hadoop]$ [hadoop@bigdata hadoop]$ yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar wordcount /input/1.log /output1\n\n报错如下：\n/bin/bash: /bin/java: No such file or directoryFailing this attempt.Diagnostics: [2022-01-25 17:20:37.908]Exception from container-launch.Container id: container_1643099048243_0003_02_000001Exit code: 127[2022-01-25 17:20:37.910]Container exited with a non-zero exit code 127. Error file: prelaunch.err.Last 4096 bytes of prelaunch.err :Last 4096 bytes of stderr :/bin/bash: /bin/java: No such file or directory[2022-01-25 17:20:37.911]Container exited with a non-zero exit code 127. Error file: prelaunch.err.Last 4096 bytes of prelaunch.err :Last 4096 bytes of stderr :/bin/bash: /bin/java: No such file or directory\n\n解决办法：创建软链接，指向 /bin/java（root用户）\n[root@bigdata ~]# ln -s /usr/java/jdk1.8.0_45/bin/java /bin/java\n\n4.4 提交 yarn 任务yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar wordcount /input/1.log /output1查看计算结果[hadoop@bigdata hadoop]$ hdfs dfs -cat /output1/part-r-00000\tDiego\t2\tJodie\t3\tspark\t2\tworld\t1[hadoop@bigdata hadoop]$\n\n至此，yarn 启动、测试完成。\n","tags":["hadoop","yarn"]},{"title":"自购阿里云服务器遭恶意挖矿如何自救","url":"/2020/04/17/%E8%87%AA%E8%B4%AD%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%81%AD%E6%81%B6%E6%84%8F%E6%8C%96%E7%9F%BF%E5%A6%82%E4%BD%95%E8%87%AA%E6%95%91/","content":"如果在未购买云安全中心服务的情况下遇到挖矿病毒，可以采取如下措施排查和处理（本操作以Linux系统为例）：\n\n查看挖矿进程的执行文件链接。\nls -l /proc/xxx/exe           // xxx表示该进程的PID。\n清除挖矿进程的执行文件。\nrm -rf /home/hadoop/app/hadoop-3.2.2/sbin/ax957x/kthreaddk\n在高CPU消耗（比如：398%）的进程中定位到挖矿进程，并杀死该进程。\nkill -9 85469\n检查您服务器的防火墙中是否存在挖矿程序的矿池地址。\n\n执行以下命令查看是否存在业务范围之外的可疑通信地址和开放端口。\niptables -L -n\n执行以下命令清除恶意矿池地址。\nvi /etc/sysconfig/iptables\n\n\n执行以下命令排查是否存在定时任务。\ncrontab -l\n\n您可以根据排查的结果，对可疑的定时任务文件进行处理，防止二次入侵。\n# 编辑定时任务，删除crontab -e# 删除恶意文件定时任务crontab -r\n执行以下命令检查SSH公钥中是否存在挖矿病毒，防止出现持续后门。\ncat .ssh/authorized_keys\n查看其他服务器中是否存在挖矿行为，防止挖矿病毒重复感染内网中的其他服务器。\n\n本人的阿里云服务器恶意程序有2个：root 用户和 hadoop 用户下都有恶意文件，分别执行上述 1 - 7 的操作，执行完成，top 命令监控 CPU，或者通过阿里云安全中心管理控制台查看是否仍有恶意程序启动。\n\n\n","tags":["阿里ECS","病毒"]}]